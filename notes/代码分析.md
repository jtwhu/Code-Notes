网络的核心代码

# 代码组成及功能分析

```python
G:.
│  .gitattributes
│  .gitignore
│  compile_op.sh                     # 编译脚本
│  helper_ply.py                     # ply文件处理相关
│  helper_requirements.txt
│  helper_tf_util.py                  # 常用函数
│  helper_tool.py                     # 超参数设置
│  jobs_6_fold_cv_s3dis.sh
│  jobs_test_semantickitti.sh         # semantickitti测试脚本
│  LICENSE
│  main_S3DIS.py                      # S3DIS主代码
│  main_Semantic3D.py                 # Semantic3D主代码
│  main_SemanticKITTI.py              # SemanticKITTI主代码
│  RandLANet.py                       # 网络核心代码
│  README_official.md
│  tester_S3DIS.py                    # S3DIS测试
│  tester_Semantic3D.py               # Semantic3D测试
│  tester_SemanticKITTI.py            # SemanticKITTI测试
└─utils
    │  6_fold_cv.py
    │  data_prepare_s3dis.py          # SemanticKITTI数据预处理
    │  data_prepare_semantic3d.py     # semantic3d数据预处理
    │  data_prepare_semantickitti.py  # semantickitti数据预处理
    │  download_semantic3d.sh         # SemanticKITTI测试
    │  semantic-kitti.yaml
    │
    ├─cpp_wrappers
    │  │  compile_wrappers.sh
    │  │
    │  ├─cpp_subsampling
    │  │  │  setup.py
    │  │  │  wrapper.cpp
    │  │  │
    │  │  └─grid_subsampling
    │  │          grid_subsampling.cpp
    │  │          grid_subsampling.h
    │  │
    │  └─cpp_utils
    │      ├─cloud
    │      │      cloud.cpp
    │      │      cloud.h
    │      │
    │      └─nanoflann
    │              nanoflann.hpp
    │
    ├─meta
    │      anno_paths.txt
    │      class_names.txt
    │
    └─nearest_neighbors
            KDTreeTableAdaptor.h
            knn.pyx
            knn_.cxx
            knn_.h
            nanoflann.hpp
            setup.py
            test.py
```

# RandLANet.py

## 代码作用

网络模型整体处理流程的构建

## 代码整体结构

![img](http://rfx1qodcu.hn-bkt.clouddn.com/1659671062316-6dc08cec-0296-4499-990a-0afb1602d2a5.jpeg)

## inference

结合配置文件分析：

- k_n = 16  # KNN 设置K近邻计算的点个数
- num_layers = 4  # Number of layers 设置层数（谁的层数？其实就是encoder和decoder的层数，二者是一样的）
- num_points = 4096 * 11  # Number of input points # 设置网络输入点数
- num_classes = 19  # Number of valid classes # 设置有效的类别个数
- sub_grid_size = 0.06  # preprocess_parameter 设置预处理下采样分辨率
- batch_size = 6  # batch_size during training 设置训练时的batch_size
- val_batch_size = 20  # batch_size during validation and test 设置验证的batch_size
- train_steps = 500  # Number of steps per epochs 设置每一轮训练的step（没太懂什么作用）
- val_steps = 100  # Number of validation steps per epoch 设置验证的step
- sub_sampling_ratio = [4, 4, 4, 4]  # 设置每一层的下采样率 sampling ratio of random sampling at each layer
- d_out = [16, 64, 128, 256]  # feature dimension 设置每一层输出的特征维度
-  num_sub_points = [num_points // 4, num_points // 16, num_points // 64, num_points // 256]     # 应该是设置每一层的点个数
- noise_init = 3.5  # noise initial parameter
- max_epoch = 100  # maximum epoch during training
- learning_rate = 1e-2  # initial learning rate
- lr_decays = {i: 0.95 for i in range(0, 500)}  # decay rate of learning rate
- train_sum_dir = 'train_log'
- saving = True
-  saving_path = None

```python
class ConfigSemanticKITTI:
    k_n = 16  # KNN 设置K近邻计算的点个数
    num_layers = 4  # Number of layers 设置层数（谁的层数？）
    num_points = 4096 * 11  # Number of input points # 设置网络输入点数
    num_classes = 19  # Number of valid classes # 设置有效的类别个数
    sub_grid_size = 0.06  # preprocess_parameter 设置预处理下采样分辨率

    batch_size = 6  # batch_size during training 设置训练时的batch_size
    val_batch_size = 20  # batch_size during validation and test 设置验证的batch_size
    train_steps = 500  # Number of steps per epochs 设置每一轮训练的step（没太懂什么作用）
    val_steps = 100  # Number of validation steps per epoch 设置验证的step

    sub_sampling_ratio = [4, 4, 4, 4]  # 设置每一层的下采样率 sampling ratio of random sampling at each layer
    d_out = [16, 64, 128, 256]  # feature dimension 设置每一层输出的特征维度
    # 应该是设置每一层的点个数
    num_sub_points = [num_points // 4, num_points // 16, num_points // 64, num_points // 256]

    noise_init = 3.5  # noise initial parameter
    max_epoch = 100  # maximum epoch during training
    learning_rate = 1e-2  # initial learning rate
    lr_decays = {i: 0.95 for i in range(0, 500)}  # decay rate of learning rate

    train_sum_dir = 'train_log'
    saving = True
    saving_path = None
```

- infer.py

![img](http://rfx1qodcu.hn-bkt.clouddn.com/1659750822466-f10ebad9-d5a5-4aed-82e3-7228d33dcab9.jpeg)

```python
 # 网络处理流程
    def inference(self, inputs, is_training):

        d_out = self.config.d_out # 每一层输出特征的通道数，由配置文件进行设置
        feature = inputs['features']
        # 下面三个是一个MLP结构，将特征通道数转换为8
        feature = tf.layers.dense(feature, 8, activation=None, name='fc0')# 全连接层， B, N ,3+x => B, N, 8
        feature = tf.nn.leaky_relu(tf.layers.batch_normalization(feature, -1, 0.99, 1e-6, training=is_training))
        feature = tf.expand_dims(feature, axis=2)# 在x轴处增加一个维度

        # ###########################Encoder############################
        f_encoder_list = []
        for i in range(self.config.num_layers):# 其实就是encoder的层数
            # 结合文章看更加直观
            # LFA模块输出（B, N, 2*d_out）
            f_encoder_i = self.dilated_res_block(feature, inputs['xyz'][i], inputs['neigh_idx'][i], d_out[i],
                                                 'Encoder_layer_' + str(i), is_training)
            # RS模块
            f_sampled_i = self.random_sample(f_encoder_i, inputs['sub_idx'][i])
            feature = f_sampled_i
            if i == 0:
                f_encoder_list.append(f_encoder_i) # 将计算结果进行保存，用于后续跳跃链接使用
            f_encoder_list.append(f_sampled_i)
        # ###########################Encoder############################

        feature = helper_tf_util.conv2d(f_encoder_list[-1], f_encoder_list[-1].get_shape()[3].value, [1, 1],
                                        'decoder_0',
                                        [1, 1], 'VALID', True, is_training)

        # ###########################Decoder############################
        f_decoder_list = []
        for j in range(self.config.num_layers):
            # 最近邻插值上采样
            f_interp_i = self.nearest_interpolation(feature, inputs['interp_idx'][-j - 1])
            # 反卷积过程，注意这里的跨层连接过程
            f_decoder_i = helper_tf_util.conv2d_transpose(tf.concat([f_encoder_list[-j - 2], f_interp_i], axis=3),
                                                          f_encoder_list[-j - 2].get_shape()[-1].value, [1, 1],
                                                          'Decoder_layer_' + str(j), [1, 1], 'VALID', bn=True,
                                                          is_training=is_training)
            feature = f_decoder_i
            f_decoder_list.append(f_decoder_i)
        # ###########################Decoder############################


        # 网络的分割模块是由三个MLP+dropout组成
        f_layer_fc1 = helper_tf_util.conv2d(f_decoder_list[-1], 64, [1, 1], 'fc1', [1, 1], 'VALID', True, is_training)
        f_layer_fc2 = helper_tf_util.conv2d(f_layer_fc1, 32, [1, 1], 'fc2', [1, 1], 'VALID', True, is_training)
        f_layer_drop = helper_tf_util.dropout(f_layer_fc2, keep_prob=0.5, is_training=is_training, scope='dp1')
        # 最终对应到分类个数，得到输出结果得分
        f_layer_fc3 = helper_tf_util.conv2d(f_layer_drop, self.config.num_classes, [1, 1], 'fc', [1, 1], 'VALID', False,
                                            is_training, activation_fn=None)
        f_out = tf.squeeze(f_layer_fc3, [2])
        return f_out
```

- dilated_res_block

- - 结构分析

- - - 一个MLP(N，din) =>(N, dout/2)
    - 残差结构残差边

- - - - 接一个MLP(N， dout/2)==>(N, 2xdout)

- - - 残差结构非残差边

- - - - LocSE()
      - atttentive

结合网络流程图看代码更加直观

![img](http://rfx1qodcu.hn-bkt.clouddn.com/v2-12bee93d12fc6f3da2d95c18311fcaad_r-1659185678148100.jpg)

**LFA**

![img](http://rfx1qodcu.hn-bkt.clouddn.com/1659704873856-7ada61ec-a999-444c-9cb6-a2e322f87f3d.png)

**网络整体流程**

![img](http://rfx1qodcu.hn-bkt.clouddn.com/1659705175500-24c5752d-b72e-4c74-8472-00393e58c95f.jpeg)

### dilated residual block(扩张残差块代码实现)

```python
    # 扩张残差块代码实现
    # 通过扩张残差块实现局部特征聚合（LFA）, 扩张残差块包含两个组件：
    # 扩张残差块可以有效增大点的感受野，同时保证较小的计算量（2个localSE和Attentive Pooling效果最好）
    def dilated_res_block(self, feature, xyz, neigh_idx, d_out, name, is_training):
        # 第一个MLP，对照网络结构图看就可以了
        f_pc = helper_tf_util.conv2d(feature, d_out // 2, [1, 1], name + 'mlp1', [1, 1], 'VALID', True, is_training)
        # 扩张残差块基本结构
        f_pc = self.building_block(xyz, f_pc, neigh_idx, d_out, name + 'LFA', is_training)
        f_pc = helper_tf_util.conv2d(f_pc, d_out * 2, [1, 1], name + 'mlp2', [1, 1], 'VALID', True, is_training,
                                     activation_fn=None)
        # 残差边，直接MLP
        shortcut = helper_tf_util.conv2d(feature, d_out * 2, [1, 1], name + 'shortcut', [1, 1], 'VALID',
                                         activation_fn=None, bn=True, is_training=is_training)
        # 残差结果和MLP结果进行激活 
        return tf.nn.leaky_relu(f_pc + shortcut)
```

### building block（两次locSE和attentive pooling）

```python
    def building_block(self, xyz, feature, neigh_idx, d_out, name, is_training):
        d_in = feature.get_shape()[-1].value
        # ==========================local spacial embeding==========================
        f_xyz = self.relative_pos_encoding(xyz, neigh_idx)# 邻域相对位置特征编码(B,N,K,D)
        # 注意计算完邻域内信息之后需要经过MLP,调整通道数
        # 经过MLP只有的f_xyz就是最终的邻域相对位置特征了，下面就可以和邻域语义特征concat
        f_xyz = helper_tf_util.conv2d(f_xyz, d_in, [1, 1], name + 'mlp1', [1, 1], 'VALID', True, is_training)
        f_neighbours = self.gather_neighbour(tf.squeeze(feature, axis=2), neigh_idx)
        f_concat = tf.concat([f_neighbours, f_xyz], axis=-1)
        # ==========================attentive pooling =====================================
        f_pc_agg = self.att_pooling(f_concat, d_out // 2, name + 'att_pooling_1', is_training)

        # ==========================local spacial embeding(没有进行相对位置信息编码，而是直接接入MLP，和网络示意图有出入)==========================
        f_xyz = helper_tf_util.conv2d(f_xyz, d_out // 2, [1, 1], name + 'mlp2', [1, 1], 'VALID', True, is_training)
        f_neighbours = self.gather_neighbour(tf.squeeze(f_pc_agg, axis=2), neigh_idx)
        f_concat = tf.concat([f_neighbours, f_xyz], axis=-1)
        f_pc_agg = self.att_pooling(f_concat, d_out, name + 'att_pooling_2', is_training)
        return f_pc_agg
```

### 